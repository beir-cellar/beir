from __future__ import annotations

from collections.abc import Iterable

from torch import Tensor, nn


class MarginMSELoss(nn.Module):
    """
    Computes the Margin MSE loss between the query, positive passage and negative passage. This loss
    is used to train dense-models using cross-architecture knowledge distillation setup.

    Margin MSE Loss is defined as from (Eq.11) in Sebastian HofstÃ¤tter et al. in https://arxiv.org/abs/2010.02666:
    Loss(ğ‘„, ğ‘ƒ+, ğ‘ƒâˆ’) = MSE(ğ‘€ğ‘ (ğ‘„, ğ‘ƒ+) âˆ’ ğ‘€ğ‘ (ğ‘„, ğ‘ƒâˆ’), ğ‘€ğ‘¡(ğ‘„, ğ‘ƒ+) âˆ’ ğ‘€ğ‘¡(ğ‘„, ğ‘ƒâˆ’))
    where ğ‘„: Query, ğ‘ƒ+: Relevant passage, ğ‘ƒâˆ’: Non-relevant passage, ğ‘€ğ‘ : Student model, ğ‘€ğ‘¡: Teacher model

    Remember: Pass the difference in scores of the passages as labels.
    """

    def __init__(self, model, scale: float = 1.0, similarity_fct="dot"):
        super().__init__()
        self.model = model
        self.scale = scale
        self.similarity_fct = similarity_fct
        self.loss_fct = nn.MSELoss()

    def forward(self, sentence_features: Iterable[dict[str, Tensor]], labels: Tensor):
        # sentence_features: query, positive passage, negative passage
        reps = [self.model(sentence_feature)["sentence_embedding"] for sentence_feature in sentence_features]
        embeddings_query = reps[0]
        embeddings_pos = reps[1]
        embeddings_neg = reps[2]

        scores_pos = (embeddings_query * embeddings_pos).sum(dim=-1) * self.scale
        scores_neg = (embeddings_query * embeddings_neg).sum(dim=-1) * self.scale
        margin_pred = scores_pos - scores_neg

        return self.loss_fct(margin_pred, labels)
