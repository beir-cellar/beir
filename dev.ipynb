{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get queries and answers from MMLU\n",
    "mmlu_queries = {}\n",
    "mmlu_choices = {}\n",
    "mmlu_answers = {}\n",
    "mmlu_subjects = {}\n",
    "\n",
    "for i, data in enumerate(mmlu_dataset['test']):\n",
    "    mmlu_queries[i] = data['question']\n",
    "    mmlu_choices[i] = data['choices']\n",
    "    mmlu_subjects[i] = data['subject']\n",
    "    mmlu_answers[i] = data['choices'][data['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 01:52:08 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b465caf3a0a64874a94fce200cb99cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8841823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 01:52:32 - Loaded 8841823 DEV Documents.\n",
      "2025-06-03 01:52:32 - Doc Example: {'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.', 'title': ''}\n",
      "2025-06-03 01:52:32 - Loading Queries...\n",
      "2025-06-03 01:52:33 - Loaded 6980 DEV Queries.\n",
      "2025-06-03 01:52:33 - Query Example: how many years did william bradford serve as governor of plymouth colony?\n"
     ]
    }
   ],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import logging\n",
    "import pathlib, os\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#### Download scifact.zip dataset and unzip the dataset\n",
    "dataset = \"MSMARCO\"\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
    "# out_dir = os.path.join('/data/richard/taggerv2/test/test6/beir/outputs', \"datasets\")\n",
    "data_path = '/data/richard/taggerv2/test/test6/beir/outputs/datasets/msmarco'\n",
    "\n",
    "#### Provide the data_path where scifact has been downloaded and unzipped\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7067032': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(qrels.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "idx = torch.tensor([0, 1, 2])\n",
    "\n",
    "orig = [1, 2, 3, 4, 5]\n",
    "left = [orig[index] for index in idx]\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score = list(qrels.values())[123]\n",
    "doc2score = {doc_id:int(score > 0) for doc_id, score in doc2score.items()}\n",
    "doc2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(queries.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "def msmarco_collate_fn(batch):\n",
    "    # batch is a list of tuples (query, paragraphs, scores)\n",
    "    queries, paras, scores = zip(*batch)\n",
    "    return list(queries), list(paras), list(scores)\n",
    "\n",
    "class MSMARCO_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        \n",
    "        # #### Download scifact.zip dataset and unzip the dataset\n",
    "        # dataset = \"MSMARCO\"\n",
    "        # url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
    "        # # out_dir = os.path.join('/data/richard/taggerv2/test/test6/beir/outputs', \"datasets\")\n",
    "        # data_path = '/data/richard/taggerv2/test/test6/beir/outputs/datasets/msmarco'\n",
    "\n",
    "        if split == 'train':\n",
    "            corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"train\")\n",
    "        elif split == 'test':\n",
    "            corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "        self.corpus = corpus\n",
    "\n",
    "        self.corpus_keys = list(self.corpus.keys())\n",
    "        self.corpus_values = list(self.corpus.values())\n",
    "        self.qrels_keys = list(qrels.keys())\n",
    "        self.queries = list(queries.values())\n",
    "        self.qrels_values = list(qrels.values())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        query = self.queries[idx]\n",
    "        doc2score = self.qrels_values[idx]            # a dict {doc_id: score}\n",
    "        doc2score = {doc_id:int(score > 0) for doc_id, score in doc2score.items()}\n",
    "        doc_ids, scores = zip(*doc2score.items())     # two tuples\n",
    "        paragraphs     = [ self.corpus[d] for d in doc_ids ]\n",
    "        return query, list(paragraphs), list(scores)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qrels_keys)\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Instantiate the dataset (e.g. the “train” split)\n",
    "ds = MSMARCO_dataset(split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51311b7567384b3aa8ad7bab504537c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "llm_model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name, padding_side=\"left\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name).to('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from beir import util, LoggingHandler\n",
    "# from beir.retrieval import models\n",
    "# from beir.datasets.data_loader import GenericDataLoader\n",
    "# from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "# from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import torch\n",
    "\n",
    "# def llm_output(llm_model, tokenizer, queries, query_facts, \n",
    "#                max_length=256,\n",
    "#                batch_size=None, **generate_kwargs):\n",
    "#     \"\"\"\n",
    "#     Perform batch inference with a causal LLM, combining each query with its associated facts.\n",
    "\n",
    "#     Inputs:\n",
    "#         llm_model:      An accelerator-wrapped AutoModelForCausalLM already on the correct device.\n",
    "#         tokenizer:      The corresponding AutoTokenizer (not wrapped by accelerator).\n",
    "#         queries:        List[str] of length B, each a user query.\n",
    "#         query_facts:    List[List[str]] of length B, where query_facts[i] is a list of fact-strings for queries[i].\n",
    "#         max_length:     Maximum total generation length (including prompt).\n",
    "#         batch_size:     Optional int. If provided, processes inputs in chunks of this size; \n",
    "#                         otherwise, all B inputs are processed in one batch.\n",
    "#         **generate_kwargs: Additional keyword arguments to pass to llm_model.generate().\n",
    "\n",
    "#     Returns:\n",
    "#         actions: List[str] of length B, where each element is the decoded output for the corresponding query+facts.\n",
    "#     \"\"\"\n",
    "#     device = 'cuda:0'\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#     # 1. Build full prompts by concatenating each query with its facts\n",
    "#     prompts = []\n",
    "#     for q, facts in zip(queries, query_facts):\n",
    "#         # Example prompt template: you can adjust this to your preferred formatting\n",
    "#         # Here, we prefix with \"Question:\" and list facts under \"Facts:\"\n",
    "#         fact_section = \"\"\n",
    "#         if facts:\n",
    "#             fact_section = \"Facts:\\n\" + \"\\n\".join(f\"- {f}\" for f in facts) + \"\\n\"\n",
    "#         prompt = f\"Question: {q}\\n{fact_section}Answer: (Please choose from 'A, B, C, D' and answer with the letter at the very beginning or very end of you response.)\"\n",
    "#         prompts.append(prompt)\n",
    "\n",
    "#     # 2. Decide batch size\n",
    "#     B = len(prompts)\n",
    "#     if batch_size is None or batch_size >= B:\n",
    "#         batch_size = B\n",
    "\n",
    "#     all_outputs = []\n",
    "#     llm_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # 3. Process in chunks of size batch_size\n",
    "#         for i in range(0, B, batch_size):\n",
    "#             chunk_prompts = prompts[i : i + batch_size]\n",
    "\n",
    "#             # 3a. Tokenize the chunk of prompts\n",
    "#             encoding = tokenizer(\n",
    "#                 chunk_prompts,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 padding=True,\n",
    "#                 truncation=True,\n",
    "#             )\n",
    "#             input_ids = encoding[\"input_ids\"].to(device)\n",
    "#             attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "#             # 3b. Generate output IDs\n",
    "#             generated_ids = llm_model.generate(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 max_length=max_length,\n",
    "#                 pad_token_id=tokenizer.eos_token_id,\n",
    "#                 **generate_kwargs,\n",
    "#             )\n",
    "\n",
    "#             # 3c. Decode each generated sequence (skip the prompt tokens if desired)\n",
    "#             #    Here, we decode the entire generated_ids and return full text.\n",
    "#             decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "#             all_outputs.extend(decoded)\n",
    "\n",
    "#     return all_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def llm_output(llm_model, tokenizer, queries, query_facts, \n",
    "               max_length=256,\n",
    "               batch_size=None, **generate_kwargs):\n",
    "    \"\"\"\n",
    "    Perform batch inference with a causal LLM, combining each query with its associated facts.\n",
    "    Returns only the model’s generated responses (i.e., strips off the prompt).\n",
    "\n",
    "    Inputs:\n",
    "        llm_model:      An accelerator-wrapped AutoModelForCausalLM already on the correct device.\n",
    "        tokenizer:      The corresponding AutoTokenizer (not wrapped by accelerator).\n",
    "        queries:        List[str] of length B, each a user query.\n",
    "        query_facts:    List[List[str]] of length B, where query_facts[i] is a list of fact-strings for queries[i].\n",
    "        max_length:     Maximum total generation length (including prompt).\n",
    "        batch_size:     Optional int. If provided, processes inputs in chunks of this size; \n",
    "                        otherwise, all B inputs are processed in one batch.\n",
    "        **generate_kwargs: Additional keyword arguments to pass to llm_model.generate().\n",
    "\n",
    "    Returns:\n",
    "        responses: List[str] of length B, where each element is the model’s decoded output (response-only).\n",
    "    \"\"\"\n",
    "    device = llm_model.device\n",
    "    # Ensure we have a pad token to avoid generate() errors\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 1. Build full prompts by concatenating each query with its facts\n",
    "    prompts = []\n",
    "    for q, facts in zip(queries, query_facts):\n",
    "        fact_section = \"\"\n",
    "        if facts:\n",
    "            fact_section = \"Facts:\\n\" + \"\\n\".join(f\"- {f}\" for f in facts) + \"\\n\"\n",
    "        prompt = (\n",
    "            f\"Question: {q}\\n\"\n",
    "            f\"{fact_section}\"\n",
    "            \"Answer: (Please choose from 'A, B, C, D' and answer with the letter at the very beginning or very end of your response.)\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # 2. Decide batch size\n",
    "    B = len(prompts)\n",
    "    if batch_size is None or batch_size >= B:\n",
    "        batch_size = B\n",
    "\n",
    "    all_responses = []\n",
    "    llm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 3. Process in chunks of size batch_size\n",
    "        for i in range(0, B, batch_size):\n",
    "            chunk_prompts = prompts[i : i + batch_size]\n",
    "\n",
    "            # 3a. Tokenize the chunk of prompts\n",
    "            encoding = tokenizer(\n",
    "                chunk_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "            # Compute prompt token lengths from the attention mask (before moving to device)\n",
    "            prompt_lengths = (encoding[\"attention_mask\"].sum(dim=1)).tolist()\n",
    "\n",
    "            input_ids = encoding[\"input_ids\"].to(device)\n",
    "            attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "            # 3b. Generate output IDs\n",
    "            generated_ids = llm_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                **generate_kwargs,\n",
    "            )\n",
    "            # generated_ids shape: (chunk_size, seq_len_total)\n",
    "\n",
    "            # 3c. For each generated sequence, strip off prompt tokens\n",
    "            for gen_ids, prompt_len in zip(generated_ids, prompt_lengths):\n",
    "                # gen_ids: full sequence of length seq_len_total\n",
    "                continuation_ids = gen_ids[prompt_len:]\n",
    "                # Decode only the newly generated tokens\n",
    "                clean_text = tokenizer.decode(continuation_ids, skip_special_tokens=True).strip()\n",
    "                all_responses.append(clean_text)\n",
    "\n",
    "    return all_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. White sunlight\n",
      "\n",
      "Rainbows are formed when sunlight passes through water droplets in the air. The light is refracted, or bent, as it passes through the droplets, and is split into its individual colors, which are then dispersed by the shape of the droplets. This is why the colors of the rainbow always appear in the same order: red, orange, yellow, green, blue, indigo, and violet. The color of the rainbow is determined by the wavelength of the light, with red light having the longest wavelength and violet light having the shortest.\n"
     ]
    }
   ],
   "source": [
    "# Assume you already have:\n",
    "#   tokenizer = AutoTokenizer.from_pretrained(args.llm_model_name)\n",
    "#   llm_model = AutoModelForCausalLM.from_pretrained(args.llm_model_name)\n",
    "#   llm_model, optimizer, scheduler = accelerator.prepare(llm_model, optimizer, scheduler)\n",
    "\n",
    "# Suppose you have a batch of 4 queries and associated facts:\n",
    "queries = [\n",
    "    \"What causes rainbows? A. White sunlight. B. Air. C. Blue sunlight\",\n",
    "    # \"How do mitochondria generate energy?\",\n",
    "    # \"Explain the significance of the Battle of Hastings.\",\n",
    "    # \"Describe the process of polymerase chain reaction.\"\n",
    "]\n",
    "query_facts = [\n",
    "    [\"Sunlight is refracted in raindrops\", \"White light splits into colors\"],\n",
    "    # [\"Mitochondria have inner membranes\", \"Oxidative phosphorylation occurs\"],\n",
    "    # [\"Battle occurred in 1066\", \"Involved William the Conqueror and Harold Godwinson\"],\n",
    "    # [\"PCR requires cycles of heating and cooling\", \"Uses DNA polymerase enzymes\"]\n",
    "]\n",
    "\n",
    "outputs = llm_output(\n",
    "    llm_model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    queries=queries,\n",
    "    query_facts=query_facts,\n",
    "    max_length=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    batch_size=2  # process two prompts at a time\n",
    ")\n",
    "\n",
    "for out in outputs:\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. White sunlight\n",
      "\n",
      "Rainbows are formed when sunlight passes through water droplets in the air. The light is refracted, or bent, as it passes through the droplets, and is split into its individual colors, which are then dispersed by the shape of the droplets. This is why the colors of the rainbow always appear in the same order: red, orange, yellow, green, blue, indigo, and violet. The color of the rainbow is determined by the wavelength of the light, with red light having the longest wavelength and violet light having the shortest.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Pull out a single example by index\n",
    "query, paragraphs, scores = ds[5]\n",
    "\n",
    "# 3) Print or inspect\n",
    "print(\"Query:\", query)\n",
    "print(\"Number of paragraphs:\", len(paragraphs))\n",
    "print(\"Paragraph text:\", paragraphs)\n",
    "print(\"Corresponding score for first paragraph:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "# query = \"How many people live in London?\"\n",
    "# docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "#Load the model\n",
    "# model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')    # TAS-B\n",
    "# model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-v2')     # SBERT\n",
    "model = SentenceTransformer('sentence-transformers/gtr-t5-xl').to('cuda:0')      # gtr-t5-xl\n",
    "# model = SentenceTransformer('BAAI/bge-large-en-v1.5')       # BGE\n",
    "# model = SentenceTransformer('BAAI/llm-embedder')\n",
    "# model.load_state_dict(torch.load('/data/richard/taggerv2/test/test6/beir/outputs/ckpts/2025_05_27_17h55m37s/model_step_440075.pth'))\n",
    "model.load_state_dict(torch.load('/data/richard/taggerv2/test/test6/beir/outputs/ckpts/2025_05_30_21h52m36s/model_step_251471.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = '182539'\n",
    "query = queries[q_id]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for key, val in qrels[q_id].items():\n",
    "    if val == 1:\n",
    "        docs.append(corpus[key]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode query and documents\n",
    "query_emb = model.encode(query)\n",
    "doc_emb = model.encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# assume scores is your [200×8e6] tensor\n",
    "scores = torch.randn(200, 8_000_000, device='cuda')\n",
    "\n",
    "# topk returns (values, indices); we only need indices here\n",
    "_, top10_idx = scores.topk(k=10, dim=1, largest=True, sorted=True)\n",
    "\n",
    "print(top10_idx.shape)   # torch.Size([200, 10])\n",
    "# top10_idx[i] is a length-10 LongTensor of column-IDs for the i-th row’s top scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#### Download scifact.zip dataset and unzip the dataset\n",
    "dataset = \"MSMARCO\"\n",
    "data_path = '/data/richard/taggerv2/test/test6/beir/outputs/datasets/msmarco'\n",
    "\n",
    "#### Provide the data_path where scifact has been downloaded and unzipped\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(corpus.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(qrels.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# query = \"How many people live in London?\"\n",
    "# docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "input_queries = list(queries.values())\n",
    "input_docs = list(corpus.values())\n",
    "\n",
    "#Load the model\n",
    "# model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')    # TAS-B\n",
    "# model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-v2')     # SBERT\n",
    "model = SentenceTransformer('sentence-transformers/gtr-t5-xl').to('cuda:0')      # gtr-t5-xl\n",
    "# model = SentenceTransformer('BAAI/bge-large-en-v1.5')       # BGE\n",
    "\n",
    "#Encode query and documents\n",
    "query_emb = model.encode(input_queries)\n",
    "doc_emb = model.encode(input_docs)\n",
    "print(doc_emb.shape)\n",
    "raise ValueError\n",
    "\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import logging\n",
    "import pathlib, os\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#### Download scifact.zip dataset and unzip the dataset\n",
    "dataset = \"MSMARCO\"\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
    "# out_dir = os.path.join('/data/richard/taggerv2/test/test6/beir/outputs', \"datasets\")\n",
    "data_path = '/data/richard/taggerv2/test/test6/beir/outputs/datasets/msmarco'\n",
    "\n",
    "#### Provide the data_path where scifact has been downloaded and unzipped\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "#### Load the SBERT model and retrieve using cosine-similarity\n",
    "# model = DRES(models.SentenceBERT(\"Alibaba-NLP/gte-modernbert-base\"), batch_size=16)\n",
    "# model = DRES(models.SentenceBERT(\"msmarco-roberta-base-ance-firstp\"))\n",
    "model = DRES(models.SentenceBERT('sentence-transformers/gtr-t5-xl'))\n",
    "\n",
    "#### Or load models directly from HuggingFace\n",
    "# model = DRES(models.HuggingFace(\n",
    "#     \"intfloat/e5-large-unsupervised\",\n",
    "#     max_length=512,\n",
    "#     pooling=\"mean\",\n",
    "#     normalize=True,\n",
    "#     prompts={\"query\": \"query: \", \"passage\": \"passage: \"}), batch_size=16)\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/gtr-t5-xl')      # gtr-t5-xl\n",
    "\n",
    "retriever = EvaluateRetrieval(model, score_function=\"cos_sim\") # or \"dot\" for dot product\n",
    "results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "#### Evaluate your model with NDCG@k, MAP@K, Recall@K and Precision@K  where k = [1,3,5,10,100,1000]\n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "print(f'ndcg, _map, recall, precision: {ndcg, _map, recall, precision}')\n",
    "mrr = retriever.evaluate_custom(qrels, results, retriever.k_values, metric=\"mrr\")\n",
    "\n",
    "### If you want to save your results and runfile (useful for reranking)\n",
    "results_dir = os.path.join(pathlib.Path(__file__).parent.absolute(), \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "#### Save the evaluation runfile & results\n",
    "util.save_runfile(os.path.join(results_dir, f\"{dataset}.run.trec\"), results)\n",
    "util.save_results(os.path.join(results_dir, f\"{dataset}.json\"), ndcg, _map, recall, precision, mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.SentenceBERT(\"Alibaba-NLP/gte-modernbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, export_optimized_onnx_model\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/gtr-t5-xl\", device='cuda:0', backend=\"onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # load (and auto-export) your ONNX model\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xl\",\n",
    "#                             device=\"cuda:0\",\n",
    "#                             backend=\"onnx\")\n",
    "\n",
    "# save the ONNX graph + config/modules to disk\n",
    "model.save_pretrained(\"./local-gtr-t5-xl-onnx\")\n",
    "\n",
    "model = SentenceTransformer(\"./local-gtr-t5-xl-onnx\", backend=\"onnx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_optimized_onnx_model(model, \"O4\", \"/data/richard/taggerv2/test/test6/onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\n",
    "    \"path/to/my/mpnet-legal-finetuned\",\n",
    "    backend=\"onnx\",\n",
    "    model_kwargs={\"file_name\": \"onnx/model_O3.onnx\"},\n",
    "    device='cuda:0'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
