{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEIR Benchmarking for Azure AI Search (Part1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate Index\n",
    "recreate_index = False\n",
    "\n",
    "# datasets to evaluate\n",
    "dataset_name = \"scifact\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variabls from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download BEIR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset_name)\n",
    "out_dir = \"./datasets\"\n",
    "data_path = util.download_and_unzip(url, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")# pull data from corpus and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "service_name = \"benchmark-ai-search\"\n",
    "index_name = dataset_name  + \"-vector\"\n",
    "\n",
    "admin_key = os.environ[\"SEARCH_ADMIN_KEY\"]\n",
    "endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "\n",
    "admin_client = SearchIndexClient(endpoint=endpoint,\n",
    "                    index_name=index_name,\n",
    "                    credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "search_client = SearchClient(endpoint=endpoint,\n",
    "                    index_name=index_name,\n",
    "                    credential=AzureKeyCredential(admin_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryCaptionResult,\n",
    "    QueryAnswerResult,\n",
    "    SemanticErrorMode,\n",
    "    SemanticErrorReason,\n",
    "    SemanticSearchResultsType,\n",
    "    QueryType,\n",
    "    VectorizedQuery,\n",
    "    VectorQuery,\n",
    "    VectorFilterMode,    \n",
    ")\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,  \n",
    "    VectorSearch,  \n",
    "    VectorSearchAlgorithmKind,  \n",
    "    VectorSearchAlgorithmMetric,  \n",
    "    VectorSearchProfile,  \n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    CorsOptions\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(QueryType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recreate_index==True:\n",
    "    try:\n",
    "        admin_client.delete_index(index_name)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        ),\n",
    "        ExhaustiveKnnAlgorithmConfiguration(\n",
    "            name=\"myExhaustiveKnn\",\n",
    "            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "            parameters=ExhaustiveKnnParameters(\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        ),\n",
    "        VectorSearchProfile(\n",
    "            name=\"myExhaustiveKnnProfile\",\n",
    "            algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SemanticConfiguration from azure ai search\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            # prioritized_keywords_fields=[SemanticField(field_name=\"Category\")],\n",
    "            content_fields=[SemanticField(field_name=\"text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_settings = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "scoring_profiles = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding data using OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version = \"2023-05-15\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "model = \"text-embedding-ada-002-v2\"\n",
    "\n",
    "# Generate Document Embeddings using OpenAI Ada 002\n",
    "# Read the text-sample.json\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generate_embeddings(text, model=model):\n",
    "    return openai_client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# create documents for corpus\n",
    "documents = []\n",
    "for id in corpus:\n",
    "    #print(id)\n",
    "    documents.append({\n",
    "        \"corpusId\": id,\n",
    "        \"title\": corpus[id][\"title\"],\n",
    "        \"text\": corpus[id][\"text\"],\n",
    "        \"titleVector\": generate_embeddings(corpus[id][\"title\"]),\n",
    "        \"textVector\": generate_embeddings(corpus[id][\"text\"])\n",
    "    })\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload dataset into Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Upload documents to the index per 100 documents\n",
    "    print(\"documents size is\", len(documents))\n",
    "    if len(documents) > 1000:\n",
    "        for i in range(0, len(documents), 1000):\n",
    "            result = search_client.upload_documents(documents=documents[i:i+1000])\n",
    "            print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "    else:\n",
    "            result = search_client.upload_documents(documents=documents)\n",
    "            print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "except Exception as e:\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full text search (simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.\"\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query, \n",
    "    include_total_count=True, \n",
    "    top=5,\n",
    "    query_type=\"simple\")\n",
    "for result in results:\n",
    "    print(result[\"@search.score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.\"\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query, \n",
    "    include_total_count=True, \n",
    "    top=5,\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name='my-semantic-config')\n",
    "for result in results:\n",
    "    print(result[\"@search.score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hnsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.\"\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=generate_embeddings(query),\n",
    "    k_nearest_neighbors=3,\n",
    "    fields=\"titleVector, textVector\",\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None, \n",
    "    vector_queries=[vector_query], \n",
    "    include_total_count=True, \n",
    "    top=5)\n",
    "for result in results:\n",
    "    print(result[\"@search.score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exhausive KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.\"\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=generate_embeddings(query),\n",
    "    k_nearest_neighbors=3,\n",
    "    fields=\"titleVector, textVector\",\n",
    "    exhaustive=True\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None, \n",
    "    vector_queries=[vector_query], \n",
    "    include_total_count=True, \n",
    "    top=5,\n",
    "    )\n",
    "for result in results:\n",
    "    print(result[\"@search.score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.\"\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=generate_embeddings(query),\n",
    "    k_nearest_neighbors=3,\n",
    "    fields=\"titleVector, textVector\",\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query, \n",
    "    vector_queries=[vector_query], \n",
    "    include_total_count=True, \n",
    "    top=5,\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name='my-semantic-config')\n",
    "for result in results:\n",
    "    print(result[\"@search.score\"])\n",
    "    print(result[\"@search.reranker_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from beir import util, LoggingHandler\n",
    "# from beir.datasets.data_loader import GenericDataLoader\n",
    "# from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "# from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "\n",
    "\n",
    "# # %%\n",
    "# import logging\n",
    "\n",
    "\n",
    "# def run(dataset_name):\n",
    "#     # %%\n",
    "#     print(\"dataset is \", dataset_name)\n",
    "#     dataset = dataset_name\n",
    "#     url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "#     out_dir = \"./datasets\"\n",
    "#     data_path = util.download_and_unzip(url, out_dir)\n",
    "\n",
    "#     # %%\n",
    "#     corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")# pull data from corpus and queries\n",
    "\n",
    "#     # %%\n",
    "#     from azure.core.credentials import AzureKeyCredential\n",
    "#     from azure.search.documents.indexes import SearchIndexClient \n",
    "#     from azure.search.documents import SearchClient\n",
    "\n",
    "\n",
    "#     service_name = \"benchmark-ai-search\"\n",
    "#     admin_key = os.environ[\"SEARCH_ADMIN_KEY\"]\n",
    "\n",
    "#     index_name = dataset  + \"-vector\"\n",
    "\n",
    "#     # Create an SDK client\n",
    "#     endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "#     admin_client = SearchIndexClient(endpoint=endpoint,\n",
    "#                         index_name=index_name,\n",
    "#                         credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "#     search_client = SearchClient(endpoint=endpoint,\n",
    "#                         index_name=index_name,\n",
    "#                         credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "#     try:\n",
    "#         result = admin_client.delete_index(index_name)\n",
    "#         print ('Index', index_name, 'Deleted')\n",
    "#     except Exception as ex:\n",
    "#         print (ex)\n",
    "\n",
    "#     # %%\n",
    "#     # from azure.search.documents.models import (\n",
    "#     #     QueryAnswerType,\n",
    "#     #     QueryCaptionType,\n",
    "#     #     QueryLanguage,\n",
    "#     #     QueryType,\n",
    "#     #     RawVectorQuery,\n",
    "#     #     VectorizableTextQuery,\n",
    "#     #     VectorFilterMode,    \n",
    "#     # )\n",
    "#     from azure.search.documents.models import (\n",
    "#         QueryAnswerType,\n",
    "#         QueryCaptionType,\n",
    "#         QueryCaptionResult,\n",
    "#         QueryAnswerResult,\n",
    "#         SemanticErrorMode,\n",
    "#         SemanticErrorReason,\n",
    "#         SemanticSearchResultsType,\n",
    "#         QueryType,\n",
    "#         VectorizedQuery,\n",
    "#         VectorQuery,\n",
    "#         VectorFilterMode,    \n",
    "#     )\n",
    "#     from azure.search.documents.indexes.models import (  \n",
    "#         # AzureOpenAIEmbeddingSkill,  \n",
    "#         # AzureOpenAIParameters,  \n",
    "#         # AzureOpenAIVectorizer,  \n",
    "#         # ExhaustiveKnnParameters,  \n",
    "#         # ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
    "#         ExhaustiveKnnAlgorithmConfiguration,\n",
    "#         ExhaustiveKnnParameters,\n",
    "#         FieldMapping,  \n",
    "#         # HnswParameters,  \n",
    "#         # HnswVectorSearchAlgorithmConfiguration,\n",
    "#         HnswAlgorithmConfiguration,\n",
    "#         HnswParameters,\n",
    "#         # IndexProjectionMode,  \n",
    "#         # InputFieldMappingEntry,  \n",
    "#         # OutputFieldMappingEntry,  \n",
    "#         # SemanticPrioritizedFieldsPrioritizedFields,    \n",
    "#         SearchField,  \n",
    "#         SearchFieldDataType,  \n",
    "#         SearchIndex,  \n",
    "#         SearchIndexer,  \n",
    "#         SearchIndexerDataContainer,  \n",
    "#         SearchIndexerDataSourceConnection,  \n",
    "#         # SearchIndexerIndexProjectionSelector,  \n",
    "#         # SearchIndexerIndexProjections,  \n",
    "#         # SearchIndexerIndexProjectionsParameters,  \n",
    "#         SearchIndexerSkillset,  \n",
    "#         SemanticConfiguration,  \n",
    "#         SemanticField,  \n",
    "#         # SemanticSettings,  \n",
    "#         SplitSkill,  \n",
    "#         VectorSearch,  \n",
    "#         VectorSearchAlgorithmKind,  \n",
    "#         VectorSearchAlgorithmMetric,  \n",
    "#         VectorSearchProfile,  \n",
    "#         SimpleField,\n",
    "#         SearchableField,\n",
    "#         CorsOptions,\n",
    "#         SemanticSearch,\n",
    "#         SemanticPrioritizedFields,\n",
    "#     ) \n",
    "\n",
    "#     # Configure the vector search configuration  \n",
    "#     vector_search = VectorSearch(\n",
    "#         algorithms=[\n",
    "#             HnswAlgorithmConfiguration(\n",
    "#                 name=\"myHnsw\",\n",
    "#                 kind=VectorSearchAlgorithmKind.HNSW,\n",
    "#                 parameters=HnswParameters(\n",
    "#                     m=4,\n",
    "#                     ef_construction=400,\n",
    "#                     ef_search=500,\n",
    "#                     metric=VectorSearchAlgorithmMetric.COSINE\n",
    "#                 )\n",
    "#             ),\n",
    "#             ExhaustiveKnnAlgorithmConfiguration(\n",
    "#                 name=\"myExhaustiveKnn\",\n",
    "#                 kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "#                 parameters=ExhaustiveKnnParameters(\n",
    "#                     metric=VectorSearchAlgorithmMetric.COSINE\n",
    "#                 )\n",
    "#             )\n",
    "#         ],\n",
    "#         profiles=[\n",
    "#             VectorSearchProfile(\n",
    "#                 name=\"myHnswProfile\",\n",
    "#                 algorithm_configuration_name=\"myHnsw\",\n",
    "#             ),\n",
    "#             VectorSearchProfile(\n",
    "#                 name=\"myExhaustiveKnnProfile\",\n",
    "#                 algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     fields = [\n",
    "#         SimpleField(name=\"corpusId\", type=SearchFieldDataType.String, key=True),\n",
    "#         SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "#         SearchableField(name=\"text\", type=SearchFieldDataType.String),\n",
    "#         SearchField(name=\"titleVector\", type=SearchFieldDataType.Collection(\n",
    "#             SearchFieldDataType.Single), \n",
    "#             vector_search_dimensions=1536, \n",
    "#             vector_search_profile_name=\"myHnswProfile\"), \n",
    "#         SearchField(name=\"textVector\", type=SearchFieldDataType.Collection(\n",
    "#             SearchFieldDataType.Single), \n",
    "#             vector_search_dimensions=1536, \n",
    "#             vector_search_profile_name=\"myHnswProfile\"),  \n",
    "#     ]\n",
    "\n",
    "#     # import SemanticConfiguration from azure ai search\n",
    "#     semantic_config = SemanticConfiguration(\n",
    "#         name=\"my-semantic-config\",\n",
    "#         prioritized_fields=SemanticPrioritizedFields(\n",
    "#                 title_field=SemanticField(field_name=\"title\"),\n",
    "#                 # prioritized_keywords_fields=[SemanticField(field_name=\"Category\")],\n",
    "#                 content_fields=[SemanticField(field_name=\"text\")]\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     semantic_settings = SemanticSearch(configurations=[semantic_config])\n",
    "#     cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "#     scoring_profiles = []\n",
    "#     index = SearchIndex(\n",
    "#         name=index_name,\n",
    "#         fields=fields,\n",
    "#         vector_search=vector_search,\n",
    "#         semantic_search=semantic_settings,\n",
    "#         scoring_profiles=scoring_profiles,\n",
    "#         cors_options=cors_options)\n",
    "\n",
    "#     print(index)\n",
    "\n",
    "#     try:\n",
    "#         result = admin_client.create_index(index)\n",
    "#         print ('Index', result.name, 'created')\n",
    "#     except Exception as ex:\n",
    "#         print ('Index creation error:', ex)\n",
    "\n",
    "#     # %%\n",
    "#     import openai\n",
    "#     from openai import AzureOpenAI\n",
    "#     from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "\n",
    "#     openai_client = AzureOpenAI(\n",
    "#         api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "#         api_version = \"2023-05-15\",\n",
    "#         azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "#     )\n",
    "\n",
    "#     model = \"text-embedding-ada-002-v2\"\n",
    "\n",
    "#     # Generate Document Embeddings using OpenAI Ada 002\n",
    "#     # Read the text-sample.json\n",
    "#     @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "#     # Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "#     def generate_embeddings(text, model=model):\n",
    "#         return openai_client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "#     # create documents for corpus\n",
    "#     documents = []\n",
    "#     for id in corpus:\n",
    "#         #print(id)\n",
    "#         documents.append({\n",
    "#             \"corpusId\": id,\n",
    "#             \"title\": corpus[id][\"title\"],\n",
    "#             \"text\": corpus[id][\"text\"],\n",
    "#             \"titleVector\": generate_embeddings(corpus[id][\"title\"]),\n",
    "#             \"textVector\": generate_embeddings(corpus[id][\"text\"])\n",
    "#         })\n",
    "\n",
    "\n",
    "#     # %%\n",
    "#     try:\n",
    "#         # Upload documents to the index per 100 documents\n",
    "#         print(\"documents size is\", len(documents))\n",
    "#         if len(documents) > 1000:\n",
    "#             for i in range(0, len(documents), 1000):\n",
    "#                 result = search_client.upload_documents(documents=documents[i:i+1000])\n",
    "#                 print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "#         else:\n",
    "#                 result = search_client.upload_documents(documents=documents)\n",
    "#         #result = search_client.upload_documents(documents=documents)\n",
    "#         # print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "#     except Exception as ex:\n",
    "#         print (ex.message)\n",
    "\n",
    "\n",
    "#     # %% [markdown]\n",
    "#     # ## Search an index\n",
    "#     import time\n",
    "#     time.sleep(30)\n",
    "#     # %%\n",
    "#     results = search_client.search(search_text=\"*\", include_total_count=True)\n",
    "#     print ('Total Documents Matching Query:', results.get_count())\n",
    "#     # for result in results:\n",
    "#     #     print(result)\n",
    "    \n",
    "#     query = \"Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.\"\n",
    "#     vector_query = VectorizedQuery(\n",
    "#         vector=generate_embeddings(query),\n",
    "#         k_nearest_neighbors=3,\n",
    "#         fields=\"titleVector\",\n",
    "#     )\n",
    "    \n",
    "#     results = search_client.search(\n",
    "#         search_text=None, \n",
    "#         vector_queries=[vector_query], \n",
    "#         include_total_count=True, \n",
    "#         top=5)\n",
    "#     for result in results:\n",
    "#         print(result[\"@search.score\"])\n",
    "#     # %%\n",
    "\n",
    "#     # print(\"I like drug... with semantic search\")\n",
    "#     # results = search_client.search(search_text=\"I like drug.\", include_total_count=True, select='corpusId, title, text', top=5, semantic_configuration_name=\"my-semantic-config\", query_type=\"semantic\")\n",
    "#     # print('Total Documents Matching Query:', results.get_count())\n",
    "#     # for result in results:\n",
    "#     #     print(result.keys())\n",
    "#     #     print(\"@score:\", result[\"@search.score\"])\n",
    "#     #     print(\"@score.reranker_score\", result[\"@search.reranker_score\"])\n",
    "#     #     print(\"corpusId:\", result[\"corpusId\"])\n",
    "#     #     print(\"title\", result[\"title\"])\n",
    "#     #     print(\"text\", result[\"text\"][:100], \"...\\n\")\n",
    "\n",
    "\n",
    "#     # print(\"I like drug... without semantic search\")\n",
    "#     # results = search_client.search(search_text=\"I like drug.\", include_total_count=True, select='corpusId, title, text', top=5)\n",
    "#     # print ('Total Documents Matching Query:', results.get_count())\n",
    "#     # for result in results:\n",
    "#     #     print(\"@score:\", result[\"@search.score\"])\n",
    "#     #     print(\"@score.reranker_score\", result[\"@search.reranker_score\"])\n",
    "#     #     print(\"corpusId:\", result[\"corpusId\"])\n",
    "#     #     print(\"title\", result[\"title\"])\n",
    "#     #     print(\"text\", result[\"text\"][:100], \"...\\n\")\n",
    "\n",
    "#     # %%\n",
    "#     # query_ids = list(queries)\n",
    "#     # dict_results = {}\n",
    "#     # for query_id in query_ids:\n",
    "#     #     query = queries[query_id]\n",
    "#     #     results = search_client.search(search_text=query, include_total_count=True, select='corpusId, title, text', top=50)\n",
    "#     #     id_score = {}\n",
    "#     #     for result in results:\n",
    "#     #         id_score[result[\"corpusId\"]] = result[\"@search.score\"]\n",
    "#     #     # print(id_score)\n",
    "#     #     dict_results[query_id] = id_score\n",
    "\n",
    "#     # # %%\n",
    "#     # from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "#     # ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, dict_results, [1, 3, 5, 10, 50, 100])\n",
    "#     # print(ndcg, _map, recall, precision)\n",
    "\n",
    "#     # import json\n",
    "#     # with open(dataset+\"_azureaisearch_no_reranking_result.json\", \"w\") as f:\n",
    "#     #     json.dump(dict_results, f)\n",
    "\n",
    "\n",
    "\n",
    "#     # query_ids = list(queries)\n",
    "#     # dict_results = {}\n",
    "#     # for query_id in query_ids:\n",
    "#     #     query = queries[query_id]\n",
    "#     #     results = search_client.search(search_text=query, include_total_count=True, select='corpusId, title, text', top=50, semantic_configuration_name=\"my-semantic-config\", query_type=\"semantic\")\n",
    "#     #     # results = search_client.search(search_text=query, include_total_count=True, select='corpusId, title, text', top=100, semantic_configuration_name=\"my-semantic-config\", query_type=\"semantic\")\n",
    "#     #     id_score = {}\n",
    "#     #     for result in results:\n",
    "#     #         # print(\"query:\", query)\n",
    "#     #         # print(result[\"@search.reranker_score\"])\n",
    "#     #         id_score[result[\"corpusId\"]] = result[\"@search.reranker_score\"]\n",
    "#     #     # print(id_score)\n",
    "#     #     dict_results[query_id] = id_score\n",
    "\n",
    "#     # # %%\n",
    "#     # from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "#     # ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, dict_results, [1, 3, 5, 10, 50, 100])\n",
    "#     # print(ndcg, _map, recall, precision)\n",
    "#     # with open(dataset+\"_azureaisearch_reranking_result.json\", \"w\") as f:\n",
    "#     #     json.dump(dict_results, f)\n",
    "\n",
    "# # %%\n",
    "\n",
    "# def main():\n",
    "#     print(\"hello\")\n",
    "#     load_dotenv()\n",
    "#     logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "#                     datefmt='%Y-%m-%d %H:%M:%S',\n",
    "#                     level=logging.CRITICAL,\n",
    "#                     handlers=[LoggingHandler()])\n",
    "#     # dataset_names = [\"scifact\", \"nq\", \"scidocs\", \"arguana\", \"climate-fever\", \"dbpedia\", \"fever\", \"hotpotqa\", \"covid\", \"touche2020\"]\n",
    "#     dataset_names = [\"scifact\"]\n",
    "#     # dataset_names = [\"scifact\", \"scidocs\", \"arguana\", \"climate-fever\"]\n",
    "\n",
    "#     for dataset_name in dataset_names:\n",
    "#         run(dataset_name)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
