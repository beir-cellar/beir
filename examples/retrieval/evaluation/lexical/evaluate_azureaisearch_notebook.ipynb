{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/beir/lib/python3.8/site-packages/beir/util.py:2: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.CRITICAL,\n",
    "                    handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"hotpotqa\"\n",
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "out_dir = \"./datasets\"\n",
    "data_path = util.download_and_unzip(url, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5233329/5233329 [00:24<00:00, 210394.10it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")# pull data from corpus and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SEARCH_ADMIN_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msearch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocuments\u001b[39;00m \u001b[39mimport\u001b[39;00m SearchClient\n\u001b[1;32m      6\u001b[0m service_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfta-ai-search\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m admin_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mSEARCH_ADMIN_KEY\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      9\u001b[0m index_name \u001b[39m=\u001b[39m dataset\n\u001b[1;32m     11\u001b[0m \u001b[39m# Create an SDK client\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/beir/lib/python3.8/os.py:675\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    672\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SEARCH_ADMIN_KEY'"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "\n",
    "service_name = \"fta-ai-search\"\n",
    "admin_key = os.environ[\"SEARCH_ADMIN_KEY\"]\n",
    "\n",
    "index_name = dataset\n",
    "\n",
    "# Create an SDK client\n",
    "endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "admin_client = SearchIndexClient(endpoint=endpoint,\n",
    "                      index_name=index_name,\n",
    "                      credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "search_client = SearchClient(endpoint=endpoint,\n",
    "                      index_name=index_name,\n",
    "                      credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "try:\n",
    "    result = admin_client.delete_index(index_name)\n",
    "    print ('Index', index_name, 'Deleted')\n",
    "except Exception as ex:\n",
    "    print (ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    CorsOptions,\n",
    "    SearchableField,\n",
    "    SimpleField,\n",
    "    SearchIndex,\n",
    "    SearchFieldDataType\n",
    ")\n",
    "\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"corpusId\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"text\", type=SearchFieldDataType.String),\n",
    "]\n",
    "cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "scoring_profiles = []\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    scoring_profiles=scoring_profiles,\n",
    "    cors_options=cors_options)\n",
    "\n",
    "\n",
    "try:\n",
    "    result = admin_client.create_index(index)\n",
    "    print ('Index', result.name, 'created')\n",
    "except Exception as ex:\n",
    "    print (ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create documents for corpus\n",
    "documents = []\n",
    "for id in corpus:\n",
    "    #print(id)\n",
    "    documents.append({\n",
    "        \"corpusId\": id,\n",
    "        \"title\": corpus[id][\"title\"],\n",
    "        \"text\": corpus[id][\"text\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = search_client.upload_documents(documents=documents)\n",
    "    print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "except Exception as ex:\n",
    "    print (ex.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_client.search(search_text=\"*\", include_total_count=True)\n",
    "\n",
    "print ('Total Documents Matching Query:', results.get_count())\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_client.search(search_text=\"Micro\", include_total_count=True, select='corpusId, title, text')\n",
    "\n",
    "print ('Total Documents Matching Query:', results.get_count())\n",
    "# for result in results:\n",
    "#     print(\"corpusId:\", result[\"corpusId\"])\n",
    "#     print(\"title\", result[\"title\"])\n",
    "#     print(\"text\", result[\"text\"][:100], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = list(queries)\n",
    "dict_results = {}\n",
    "for query_id in query_ids:\n",
    "    query = queries[query_id]\n",
    "    results = search_client.search(search_text=query, include_total_count=True, select='corpusId, title, text', top=5)\n",
    "    id_score = {}\n",
    "    for result in results:\n",
    "        id_score[result[\"corpusId\"]] = result[\"@search.score\"]\n",
    "    # print(id_score)\n",
    "    dict_results[query_id] = id_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(qrels, dict_results, [1, 3, 5, 10, 50, 100, 1000])\n",
    "print(ndcg, _map, recall, precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
